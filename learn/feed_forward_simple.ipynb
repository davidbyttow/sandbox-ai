{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "de0226d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_derivative(x):\n",
    "    return np.where(x > 0, 1, 0)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_prime(x):\n",
    "    return sigmoid(x)*(1-sigmoid(x))\n",
    "\n",
    "def cost_derivative(y_pred, y_true):\n",
    "    return (y_pred - y_true)\n",
    "\n",
    "def mse_loss(y_true, y_pred):\n",
    "    return np.mean((y_true - y_pred) ** 2)\n",
    "\n",
    "def mse_loss_derivative(y_true, y_pred):\n",
    "    return 2 * (y_pred - y_true) / y_true.size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12fbf9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NumpyNN:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "\n",
    "        self.W1 = np.random.randn(self.input_size, self.hidden_size)\n",
    "        self.W2 = np.random.randn(self.hidden_size, self.output_size)\n",
    "        self.b1 = np.zeros((1, self.hidden_size))\n",
    "        self.b2 = np.zeros((1, self.output_size))\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.z1 = X @ self.W1 + self.b1\n",
    "        self.a1 = sigmoid(self.z1)\n",
    "        self.z2 = self.a1 @ self.W2 + self.b2\n",
    "        self.a2 = sigmoid(self.z2)\n",
    "        return self.a2\n",
    "\n",
    "    def backward(self, X, y, output):\n",
    "        self.dz2 = mse_loss_derivative(y, output) * sigmoid_prime(output)\n",
    "        self.dW2 = self.a1.T @ self.dz2\n",
    "        self.db2 = np.sum(self.dz2, axis=0)\n",
    "\n",
    "        self.dz1 = self.dz2 @ self.W2.T * sigmoid_prime(self.a1)\n",
    "        self.dW1 = X.T @ self.dz1\n",
    "        self.db1 = np.sum(self.dz1, axis=0)\n",
    "\n",
    "        self.W1 -= learning_rate * self.dW1\n",
    "        self.W2 -= learning_rate * self.dW2\n",
    "        self.b1 -= learning_rate * self.db1\n",
    "        self.b2 -= learning_rate * self.db2\n",
    "\n",
    "    def train(self, batch, epochs, learning_rate):\n",
    "        for _ in range(epochs):\n",
    "            for X, y in batch:\n",
    "                output = self.forward(X)\n",
    "                self.backward(X, y, output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf563d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BookNN:\n",
    "    def __init__(self, sizes):\n",
    "        self.num_layers = len(sizes)\n",
    "        self.sizes = sizes\n",
    "        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n",
    "        self.weights = [np.random.randn(x, y) for x, y in zip(sizes[:-1], sizes[1:])]        \n",
    "    \n",
    "    def forward(self, a):\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            a = sigmoid(a @ w + b)\n",
    "        return a\n",
    "    \n",
    "    def backprop(self, x, y):\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "\n",
    "        # feedforward\n",
    "        activation = x\n",
    "        activations = [x] # list to store all the activations, layer by layer\n",
    "        zs = [] # list to store all the z vectors, layer by layer\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            z = activation @ w + b\n",
    "            zs.append(z)\n",
    "            activation = sigmoid(z)\n",
    "            activations.append(activation)\n",
    "\n",
    "        print('>>', activations)\n",
    "            \n",
    "        # backward pass\n",
    "        delta = cost_derivative(activations[-1], y) * sigmoid_prime(zs[-1])\n",
    "        nabla_b[-1] = delta\n",
    "        nabla_w[-1] = activations[-2].transpose() @ delta\n",
    "        print(delta.shape)\n",
    "       \n",
    "        for l in range(2, self.num_layers):\n",
    "            z = zs[-l]\n",
    "            sp = sigmoid_prime(z)\n",
    "            delta = (self.weights[-l+1].transpose() @ delta) * sp\n",
    "            print(delta.shape)\n",
    "            nabla_b[-l] = delta\n",
    "            nabla_w[-l] = (activations[-l-1].transpose() @ delta)\n",
    "        return (nabla_b, nabla_w)\n",
    "                \n",
    "    def update_batch(self, batch):\n",
    "        for x, y in batch:\n",
    "            print(x, y)\n",
    "            (nabla_b, nabla_w) = self.backprop(x, y)\n",
    "            print(nabla_b, nabla_w)\n",
    "\n",
    "    def sgd(self, training_data, epochs, mini_batch_size):\n",
    "        n = len(training_data)\n",
    "        for j in range(epochs):\n",
    "            random.shuffle(training_data)\n",
    "            mini_batches = [\n",
    "                training_data[k:k+mini_batch_size]\n",
    "                for k in range(0, len(training_data), mini_batch_size)]\n",
    "            for batch in mini_batches:\n",
    "                self.update_batch(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75908d86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> [array([0, 0]), array([[0.18499073, 0.18499073, 0.18499073],\n",
      "       [0.37670336, 0.37670336, 0.37670336],\n",
      "       [0.59167034, 0.59167034, 0.59167034]]), array([[0.15797142],\n",
      "       [0.18650638],\n",
      "       [0.22304275]])]\n",
      "(3, 1)\n",
      "(3, 3)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 3 is different from 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 25\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# nn.train([(X, y)], epochs, learning_rate)\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# for x in X:\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m#     print(f\"Input: {x}, Output: {nn.forward(x)}\")\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     20\u001b[0m \n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# training_data = [(x, y) for x, y in zip(X, y)]\u001b[39;00m\n\u001b[1;32m     24\u001b[0m nn \u001b[38;5;241m=\u001b[39m BookNN([\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m---> 25\u001b[0m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackprop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# nn.sgd(training_data, epochs, 10)\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# nn.forward(X[0])\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[4], line 41\u001b[0m, in \u001b[0;36mBookNN.backprop\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28mprint\u001b[39m(delta\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     40\u001b[0m     nabla_b[\u001b[38;5;241m-\u001b[39ml] \u001b[38;5;241m=\u001b[39m delta\n\u001b[0;32m---> 41\u001b[0m     nabla_w[\u001b[38;5;241m-\u001b[39ml] \u001b[38;5;241m=\u001b[39m (\u001b[43mactivations\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43ml\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdelta\u001b[49m)\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (nabla_b, nabla_w)\n",
      "\u001b[0;31mValueError\u001b[0m: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 3 is different from 2)"
     ]
    }
   ],
   "source": [
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y = np.array([[0], [1], [1], [0]])\n",
    "\n",
    "input_size = 2\n",
    "hidden_size = 3\n",
    "output_size = 1\n",
    "epochs = 10000\n",
    "learning_rate = 0.1\n",
    "\n",
    "nn = NumpyNN(input_size, hidden_size, output_size)\n",
    "nn.forward(X)\n",
    "# nn.train([(X, y)], epochs, learning_rate)\n",
    "# for x in X:\n",
    "#     print(f\"Input: {x}, Output: {nn.forward(x)}\")\n",
    "\n",
    "\n",
    "\n",
    "# X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "# y = np.array([[0], [1], [1], [0]])\n",
    "\n",
    "# training_data = [(x, y) for x, y in zip(X, y)]\n",
    "\n",
    "\n",
    "nn = BookNN([2, 3, 1])\n",
    "nn.backprop(X[0], y[0])\n",
    "# nn.sgd(training_data, epochs, 10)\n",
    "# nn.forward(X[0])\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59b74b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y = np.array([[0], [1], [1], [0]])\n",
    "\n",
    "input_size = 2\n",
    "hidden_size = 3\n",
    "output_size = 1\n",
    "epochs = 10000\n",
    "learning_rate = 0.1\n",
    "\n",
    "nn = BookNN(input_size, hidden_size, output_size)\n",
    "nn.train([(X, y)], epochs, learning_rate)\n",
    "\n",
    "print(\"Neural network predictions:\")\n",
    "for x in X:\n",
    "    print(f\"Input: {x}, Output: {nn.forward(x)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66921176",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "\n",
    "w = np.random.randn(2, 3)\n",
    "print(w.shape)\n",
    "# a = np.array(([0, 1], [0, 1]))\n",
    "# w @ a\n",
    "\n",
    "# # W1 = np.random.randn(2, 3)\n",
    "# W1 = np.array([[0, 1, 2], [1, 2, 3]])\n",
    "# W2 = np.random.randn(3, 1)\n",
    "# b1 = np.zeros((1, 3))\n",
    "# b2 = np.zeros((1, 1))\n",
    "\n",
    "# print(X)\n",
    "# print(W1)\n",
    "# print(b1.shape)\n",
    "# X @ W1 + b1\n",
    "\n",
    "# np.array([X, X] @ W1 + b1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b573defe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.25 pred: [[1.5]\n",
      " [1.5]\n",
      " [1.5]\n",
      " [1.5]]\n",
      "loss: 0.5363399301789903 pred: [[0.69479686]\n",
      " [0.42325822]\n",
      " [0.15171958]\n",
      " [0.        ]]\n",
      "loss: 0.22008356902695186 pred: [[0.53663716]\n",
      " [0.53280499]\n",
      " [0.52897283]\n",
      " [0.52514067]]\n",
      "loss: 0.23084519226880315 pred: [[0.36610805]\n",
      " [0.47738841]\n",
      " [0.5940725 ]\n",
      " [0.71075659]]\n",
      "loss: 0.26322668346092504 pred: [[0.29069779]\n",
      " [0.44018556]\n",
      " [0.58967333]\n",
      " [0.7391611 ]]\n",
      "loss: 0.2724737595339327 pred: [[0.2531872 ]\n",
      " [0.42854846]\n",
      " [0.60390972]\n",
      " [0.77927098]]\n",
      "loss: 0.2841171872693692 pred: [[0.22943437]\n",
      " [0.41619361]\n",
      " [0.60295284]\n",
      " [0.78971207]]\n",
      "loss: 0.29292572531694105 pred: [[0.21462225]\n",
      " [0.40715567]\n",
      " [0.5996891 ]\n",
      " [0.79222253]]\n",
      "loss: 0.2953277833552502 pred: [[0.20880568]\n",
      " [0.40470029]\n",
      " [0.60059491]\n",
      " [0.79648952]]\n",
      "loss: 0.2981379178275505 pred: [[0.20368101]\n",
      " [0.40181669]\n",
      " [0.60007811]\n",
      " [0.79833953]]\n"
     ]
    }
   ],
   "source": [
    "eta = 0.1\n",
    "\n",
    "input_size = 1\n",
    "hidden_size = 2\n",
    "output_size = 1\n",
    "\n",
    "# w1 = np.random.randn(input_size, hidden_size)\n",
    "# w2 = np.random.randn(hidden_size, output_size)\n",
    "\n",
    "x = np.array([[0.], [1.], [2.], [3.]])\n",
    "y = np.array([[0.], [1.], [0.], [1.]])\n",
    "w1 = np.array([[0., 1.]])\n",
    "b1 = np.array([[0.5, 0.5]])\n",
    "w2 = np.array([[1.], [0.]])\n",
    "b2 = np.array([[1.]])\n",
    "\n",
    "n = 100\n",
    "for i in range(n):\n",
    "    z1 = x @ w1 + b1\n",
    "    a1 = relu(z1)\n",
    "    z2 = a1 @ w2 + b2\n",
    "    a2 = relu(z2)\n",
    "    loss = mse_loss(1, a2)\n",
    "    if i % 10 == 0:\n",
    "        print(f'loss: {loss} pred: {a2}')\n",
    "\n",
    "    dz2 = mse_loss_derivative(y, a2) * relu_derivative(a2)\n",
    "    dw2 = a1.T @ dz2\n",
    "    db2 = np.sum(dz2, axis=0)\n",
    "\n",
    "    dz1 = dz2 @ w2.T * relu_derivative(a1)\n",
    "    dw1 = x.T @ dz1\n",
    "    db1 = np.sum(dz1, axis=0)\n",
    "\n",
    "    w1 -= eta * dw1\n",
    "    w2 -= eta * dw2\n",
    "    b1 -= eta * db1\n",
    "    b2 -= eta * db2\n",
    "\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
