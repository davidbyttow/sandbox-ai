{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "ba0adcf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 50015 characters, 76 uniques\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "data = open('./data/pg.txt', 'r').read()\n",
    "chars = list(set(data))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print(f'data has {data_size} characters, {vocab_size} uniques')\n",
    "\n",
    "ctoi = {ch:i for i, ch in enumerate(chars)}\n",
    "itoc = {i:ch for i, ch in enumerate(chars)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "cfb1d779",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "hidden_size = 100\n",
    "seq_len = 25\n",
    "learning_rate = 0.2\n",
    "\n",
    "# model params\n",
    "w_xh = np.random.randn(hidden_size, vocab_size)*0.01 # input -> hidden\n",
    "w_hh = np.random.randn(hidden_size, hidden_size)*0.01 # hidden -> hidden\n",
    "w_hy = np.random.randn(vocab_size, hidden_size)*0.01 # hidden -> output\n",
    "b_h = np.zeros((hidden_size, 1)) # hidden bias\n",
    "b_y = np.zeros((vocab_size, 1)) # output bias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "cd31a197",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss(inputs, targets, hprev):\n",
    "    \"\"\"\n",
    "    inputs,targets are both list of integers.\n",
    "    hprev is Hx1 array of initial hidden state\n",
    "    returns the loss, gradients on model parameters, and last hidden state\n",
    "    \"\"\"\n",
    "    xs, hs, ys, ps = {}, {}, {}, {}\n",
    "    hs[-1] = np.copy(hprev)\n",
    "    loss = 0\n",
    "\n",
    "    # forward pass\n",
    "    for t in range(len(inputs)):\n",
    "        # one-hot encoding\n",
    "        xs[t] = np.zeros((vocab_size, 1))\n",
    "        xs[t][inputs[t]] = 1\n",
    "        hs[t] = np.tanh(\n",
    "            np.dot(w_xh, xs[t]) + np.dot(w_hh, hs[t - 1]) + b_h\n",
    "        )  # hidden state\n",
    "        ys[t] = np.dot(w_hy, hs[t]) + b_y  # log probs for next chars\n",
    "        ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t]))  # probs for next chars\n",
    "        loss += -np.log(ps[t][targets[t], 0])  # softmax\n",
    "\n",
    "    # backward pass\n",
    "    dw_xh, dw_hh, dw_hy = np.zeros_like(w_xh), np.zeros_like(w_hh), np.zeros_like(w_hy)\n",
    "    db_h, db_y = np.zeros_like(b_h), np.zeros_like(b_y)\n",
    "    d_hnext = np.zeros_like(hs[0])\n",
    "    for t in reversed(range(len(inputs))):\n",
    "        dy = np.copy(ps[t])\n",
    "        # https://cs231n.github.io/neural-networks-case-study/#grad\n",
    "        dy[targets[t]] -= 1  # backprop into Y\n",
    "        dw_hy += np.dot(dy, hs[t].T)\n",
    "        db_y += dy\n",
    "        dh = np.dot(w_hy.T, dy) + d_hnext  # backprop into h\n",
    "        d_hraw = (1 - hs[t] * hs[t]) * dh  # backprop through tanh non-linearity\n",
    "        db_h += d_hraw\n",
    "        dw_xh += np.dot(d_hraw, xs[t].T)\n",
    "        dw_hh += np.dot(d_hraw, hs[t - 1].T)\n",
    "        d_hnext = np.dot(w_hh.T, d_hraw)\n",
    "\n",
    "    for dparam in [dw_xh, dw_hh, dw_hy, db_h, db_y]:\n",
    "        np.clip(dparam, -5, 5, out=dparam)  # prevent exploding gradients\n",
    "    return loss, dw_xh, dw_hh, dw_hy, db_h, db_y, hs[len(inputs) - 1]\n",
    "\n",
    "\n",
    "def sample(h, seed_ix, n):\n",
    "    \"\"\"\n",
    "    sample a sequence of integers from the model\n",
    "    h is memory state, seed_ix is seed letter for first time step\n",
    "    \"\"\"\n",
    "    x = np.zeros((vocab_size, 1))\n",
    "    x[seed_ix] = 1\n",
    "    indices = []\n",
    "    for t in range(n):\n",
    "        h = np.tanh(np.dot(w_xh, x) + np.dot(w_hh, h) + b_h)\n",
    "        y = np.dot(w_hy, h) + b_y\n",
    "        p = np.exp(y) / np.sum(np.exp(y))\n",
    "        ix = np.random.choice(range(vocab_size), p=p.ravel())\n",
    "        x = np.zeros((vocab_size, 1))\n",
    "        x[ix] = 1\n",
    "        indices.append(ix)\n",
    "    return indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "03f36a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "n, p = 0, 0\n",
    "mw_xh, mw_hh, mw_hy = np.zeros_like(w_xh), np.zeros_like(w_hh), np.zeros_like(w_hy)\n",
    "mb_h, mb_y = np.zeros_like(b_h), np.zeros_like(b_y)\n",
    "smooth_loss = -np.log(1.0/vocab_size)*seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "2001215d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      " a mo bere lostan thegfn'p bhiing startu tulipt ere fat anghacf seocore ondel't tave do it ptere to. bo o ware larke vepy meebs iser Ininzuse; thento thalrps ut sov there'lyn find what at os bis suess  \n",
      "----\n",
      "iter 0, loss: 54.84139602616046\n",
      "----\n",
      " g eithh the weit der whureuld fake to goradeiss ipisusi\"s a've thabe tipeiby €couthh any stad owh nece. ”tant tou sort thes sow pe to thesp beley surer n:uy amo ig mo theingque be dcorit pofur thertad \n",
      "----\n",
      "iter 1000, loss: 54.72965905378069\n",
      "----\n",
      " he fiicusly of at wotis tusunendtressnll mome theme. mere hony youcor nfald Tha sotu hicnyting pof numcavis more yoyers it bes bd s atnvlenn gee'rlo oplasd thallssuig ofus sald, egs mone thing mou ser \n",
      "----\n",
      "iter 2000, loss: 54.39161041173639\n",
      "----\n",
      " pal riReruny'uvocing so nreing gof the taven'l thy thre ing heyss, es suveny Aven iyece seinveit the're \"is nof to sreing forsons ich bed Allh ad thone'lpldieso'nllnt lite inco suul moumibg moo gerexp \n",
      "----\n",
      "iter 3000, loss: 54.30510841727463\n",
      "----\n",
      " hod nor cornghus yuonk, to thasus. cherseren And Thagistrpnor sotus r_era puse ununy de, So jratecuy olli-ag. yoms thony meunget Im If Thi I1;r ere I nown, exson atces e€”lart seemd cave far wo sulllp \n",
      "----\n",
      "iter 4000, loss: 54.003154823494405\n",
      "----\n",
      " rwiye blan yho'n, ins incasinbd, s ake fant pou niled ing, pomco an're and jut meed of. ipsg then astal uy wuce beainfour puthe make sor thal heift, be ohis leaks. corcolrpd lyo moing. be the il pee'r \n",
      "----\n",
      "iter 5000, loss: 53.9387295144671\n",
      "----\n",
      " he wat neys wher frave cay far,o. surtu of at to at moremo ouw usulli. se”rodiny abe pvave wrercin hat ptat of Ining to st_or If ospers eat pchove mamtu is_ dat oy row nout Yublder yoprur soure soneik \n",
      "----\n",
      "iter 6000, loss: 53.67507558603487\n",
      "----\n",
      "  hay then to tipenn lher sor nous of who pilung ouvet tha so”top, wabtame wind tik cars, oneiple wit oud mgo at'st is If is carthp Im ing if tily fod ytuld bed  farand. betiy the prealid one to it, if \n",
      "----\n",
      "iter 7000, loss: 53.638491156423896\n",
      "----\n",
      " p to cat inddarltw be the If fot dunered bethlel. cond tume ing houupe uve you thot. be there sot do in Tulale mettma€ng whex woullory. sbre is pas the to'p orele thely yeo yoolld souls. rrerpanlal do \n",
      "----\n",
      "iter 8000, loss: 53.3830166305096\n",
      "----\n",
      "  lelvemed peit y rite. in ingthes'rd usey cout nompen mike torcinenwse seinvon adippiy mptaltal lear. to lacgnd co nof xuck. tu cher hey oide expex cou iyd it theing of ise seyilly thesy the maact sub \n",
      "----\n",
      "iter 9000, loss: 53.35708084493803\n",
      "----\n",
      " h donth sores'x co thason, otu thyey it, o to mo gharing ot the soulr Anvereve weys. thad youly gont geing us ean fwratt sere do youlms reat mee wan wabe bicge soramt be to opsey creeeuy abs you sitet \n",
      "----\n",
      "iter 10000, loss: 53.1319834778331\n",
      "----\n",
      "  nowing dofile. the cobt thicifslhe cof there do asling wit' atu prout tol thyen. isidid the alt tiepcernh bo le nofos, ad thy hal one betidiyinungit don and at as ind, Diytat more at. you thisgn pral \n",
      "----\n",
      "iter 11000, loss: 53.10537776748224\n",
      "----\n",
      " he fousved thompsizowho If obe leycarusis roungre ameys yore wint sot nide. rot fanteonds tu or that ley tpreere heys ame as wot thad stuply mo whel seat souf hene vayt ase theering for've pxorre to c \n",
      "----\n",
      "iter 12000, loss: 52.909313810866834\n",
      "----\n",
      " g. to sas meamondting star tar'bke to addy pyile thecet ingell serist oy'rn vei gome foucs'rere sorte'ns beontr the tals. alce. cale do dh suve beone apstiltilee hitlvat ha”venlr to the to thrane sure \n",
      "----\n",
      "iter 13000, loss: 52.88462343617572\n",
      "----\n",
      " h whand you're tot njuplliling heor seey'e. to %orle gous of opne, the be to thes to searut sot the, onless, [eto expisidocn an cat hayun the expost vagn wax' tol omiyort a yo mereid thep ifd stey who \n",
      "----\n",
      "iter 14000, loss: 52.670356721750025\n",
      "----\n",
      "  angob. Of ngangideikee ex wanting the bes of tey of you'st moul cat or, esply wonnt. a ap to chiegsgon. mon be beering hat auls be in yus'cat doce invesn inont seen woume sus spoud seon ed intele, ha \n",
      "----\n",
      "iter 15000, loss: 52.64656538402666\n",
      "----\n",
      " he bes havey. surowh be fut do dhex'n the stu the ofy youply you that hareiyodn cobulled usuce the peint 1Tuet monat $bsch ibe omerorlans Bur con at you We suve thrand thingh Whand thyhhof ik if whof  \n",
      "----\n",
      "iter 16000, loss: 52.43479432828951\n",
      "----\n",
      " t illels. ening inti the to ison  ot icsingast isust roungasm go o amn So cgpto rerering Wher. mere mouce oy uch i ed intingsoming no. inviterad thag youm this staving the tu youmhil're ich exire, ob. \n",
      "----\n",
      "iter 17000, loss: 52.42241256586174\n",
      "----\n",
      " hed tomser tu wha cops hant mo4lyincels If Mcable dovce You to ca cone is leang soul adiye mI'tis coak't isotw selcast, to mitlveys reu do,'x whif wat youmted buld youpy cispentme mow wot A totu. the  \n",
      "----\n",
      "iter 18000, loss: 52.236853877742924\n",
      "----\n",
      " iy, being, than hee cexpemuyhico wo derig den've aw is cot Whal aspcob yourt wofons'xt froncetinged onsort bees'sygits themunes's joss than oficchesll'f loy the per te thiks, pstartt are ro] of nesing \n",
      "----\n",
      "iter 19000, loss: 52.231753683546316\n",
      "----\n",
      " he thetn't fanve thach wanco to cheop Nor as he beon.. one exper us bot I datus the. Ferere'sere goo peined preey ayce ho fame wou Pertidn. whel  lokndschs bot con starvet erstaynser thed the that hau \n",
      "----\n",
      "iter 20000, loss: 52.05969684067045\n",
      "----\n",
      " teing as anvis toâ€xiting ifha”sdom inhpred one'n of you liy to don mee stu cof, ot be of ad? youing buciad k that ney es it but bvping a to serenel do gor at. al invenky do a sot it'pd aedld gee. and \n",
      "----\n",
      "iter 21000, loss: 52.052743296654675\n",
      "----\n",
      " he tho that The jun reo yo wey orleres tho up you deledud. Poreln on moul anmer pache you brans seer I stivy. is gere, thing were goriye are leanne cThed bot lol gald Csurot So youl an at to'pen ytik. \n",
      "----\n",
      "iter 22000, loss: 51.9181705527464\n",
      "----\n",
      " d Ang store sore dermad lecactifke the'ronyt ngige thesting fiss you to serof rarige dhel onve whasd ti'st fant ral'rtiy lout cumpre wans fisereing esderig the couve qu do noigisceelle, heo the ipken  \n",
      "----\n",
      "iter 23000, loss: 51.89884615053983\n",
      "----\n",
      " he oe i, you goiyf meat. theveroing gorth'denod word the reheuis deing yoo the seat ige an on the if ist yeur oheu”s acirs on tu' ind thenering sury want sobe on stame fews dew paran sture getan wa co \n",
      "----\n",
      "iter 24000, loss: 51.70305404832782\n",
      "----\n",
      " e yeare be prore to Thtemrstidelr a sude was're tfonlor ous mea To be nto hajts wotun ilder mon, yot oens. ther nouls ile Bors mits to bspry wo bideythelsing ke abt tipecar yupoyprinld sigow Houp to l \n",
      "----\n",
      "iter 25000, loss: 51.68692308759862\n",
      "----\n",
      " he it wo ror arfangestey thach to, hin ore echa the isere sarik bethrey the one to the fi the So wel. It If to spreuke mave of dot vale are soat goume your wreGyoms migeond fiy tomeang thed tu'ncangan \n",
      "----\n",
      "iter 26000, loss: 51.51532143782937\n",
      "----\n",
      "  to in'uncicingel makowlaspthno ohs, rabsey'ume on fo geinvem mor it thons yo and the pes ine be, oa they he tancne ohe cof stupcit cof makhts omeays geap hit hob to thing ist ap amtingtisang thise al \n",
      "----\n",
      "iter 27000, loss: 51.50151187765815\n",
      "----\n",
      " h mor oneecy int gito forp butel. ntupre be moneey do gat the fortidemy jupt antme ony forid.. whint of reindswhy heson j aune firdventators caru cit geuld iccak yo anr sto than forlls. aldis'r ine th \n",
      "----\n",
      "iter 28000, loss: 51.34165986537601\n",
      "----\n",
      "  piis asivetrs. Tfu'ltuony.'t inn geuples tu eyino b higti nou thers oo aut vees to' if alleppwangs int id oandt fonan'bke hot do stusud bot inge arone iw. The dote nopery thing the arsest soress than \n",
      "----\n",
      "iter 29000, loss: 51.305038744144596\n",
      "----\n",
      " h \"rey teyeswfim You're zeamc 4] neunt've to fogs thauney reythe't as thet stured to wo thre Ther haven jur therstel'wpal gord woning thido to oferurech grup donpprere stues't reapen fon of worfrrine. \n",
      "----\n",
      "iter 30000, loss: 51.18377487432769\n",
      "----\n",
      "  woul ey robledis sotw. kw, sto ge fiy bet ofd exhe. Cthe ining to exprectel pops loure fay, parminn stiras of to gril. The of cotme the thecingw on whing corkdel thom com souson't oft a de tou sfo sp \n",
      "----\n",
      "iter 31000, loss: 51.13531283963806\n",
      "----\n",
      " he yerel ag elis in lod, itu'blld font tanx %orle moco womers hat bile,n't mes tu it youut' nist stor size ters be yourythe hecat cheohlo The amiy o're pytallcheusun'red thing pou dike't fon youpreyh' \n",
      "----\n",
      "iter 32000, loss: 51.0024667711601\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      " d wo'f and to rojt wese in pous idoystrabe ongial k'ut that aI pipvirlleey thel hacne finy jone thiy is isiPca  andfing dy yome anbe marteanve be fompanlivee ti socy ang sake ant gouut. oon Thet machh \n",
      "----\n",
      "iter 33000, loss: 50.9503153418765\n",
      "----\n",
      " he herlrandn partupsste't to ka moram then be but ga no ext ike Feres surling pup Theragen keung yourer oo nere ison the bning er, martumnevens harones be of richis pgom yourat tup. intoll mhhannorino \n",
      "----\n",
      "iter 34000, loss: 50.83430715579662\n",
      "----\n",
      "  srerell chamot i for , Tomsil. The ang maltouncer tas bucne yous ing ore the andulth being do ased youp Pour wem thiith to le the you ges who parke. forprem, thed one sospcas thter paning oneârto Hot \n",
      "----\n",
      "iter 35000, loss: 50.775898982269126\n",
      "----\n",
      " heming. I ey al Inden, it inh. le out ore wor ing I. roge er the macting at le for seor lethey Whe ithers wow mon o at o'r ami2ne to be noullded. yeat heling ankpre thy hance do disney starlchan thely \n",
      "----\n",
      "iter 36000, loss: 50.67549501531\n",
      "----\n",
      " t the the mowore Davat pamigtadn be situtid cay rored stu hes ilis nyure soiseers ham ot hermu'st no to whe. thestris, eve rois Af Tfwow thinn beciend to gosile lake. tarlas hart as cald comsury sor,  \n",
      "----\n",
      "iter 37000, loss: 50.61828281289502\n",
      "----\n",
      " he do sonyingif i dat ohe the ro theere of hato theunt g2inse'f cow I to fire to moye to thes wot foundinneles. a yot ey le go. Bust ponly ke steat stusiys whipe to critupu a diy at things thamees stu \n",
      "----\n",
      "iter 38000, loss: 50.53175928682504\n",
      "----\n",
      " d ferniy, bosiy bot cofn sike. thas oon ibeinve cor to care yourices cithing thham wo thisiy oBus to es, to'surpat Rowing st you've tha cead whand dess'xscet at yout ous at wans If countial theunt wus \n",
      "----\n",
      "iter 39000, loss: 50.453177211419366\n",
      "----\n",
      " he or roing stuaned u_pdowhoys you're yo of. if ding thy lorsing ti It dowh a compvised Nohlfing got mous in lethisowadver eond dore te whre onl any thopln cabtanly, dvenly leerelever Lecof lomey enip \n",
      "----\n",
      "iter 40000, loss: 50.37436392223697\n",
      "----\n",
      "  acting ind in be rocinfimiserte beonl whed stu haverting gugt wiat sorakes in woun at. be ane't the won ust beele at the l9upg noonels, al uus aut starltino Tfherh. Wey thaut then the byt to ancinidi \n",
      "----\n",
      "iter 41000, loss: 50.28990877966611\n",
      "----\n",
      " he of soa castnor nveroy. morre wondt soidt lone they of the whof and the, ome met you neto at o're to to gugtel pere to du rocestele he pablanay we. lors wint do wis getrat thaumivern isow a do. foul \n",
      "----\n",
      "iter 42000, loss: 50.21928539375685\n",
      "----\n",
      " d aus of'd Thyory woraton'sprast yohe of bed compcat't isod they telytrik couyeurtansting ind of tu the ellant pal coni sorely thid. If sunt stry aut, expca serro wo moobed onl rare one in saybe gis o \n",
      "----\n",
      "iter 43000, loss: 50.12836153642394\n",
      "----\n",
      " he sturo is more themom coy ge an stul you no ipe the, at hering Beers theend thonelal you hro0 ofingory fant of youst camomeycad soce netint's au wot got thy the lo ich jup ragigteald souls annt oPwh \n",
      "----\n",
      "iter 44000, loss: 50.07569231710377\n",
      "----\n",
      "  bet  The perigy to pvorli neare themis muche to is soustarpls, madsit, ath ad the tpuve to that it sores, ant thcat u. witir to wo mepeme angising of youps the has farly be then tot \"sur ann in Ind a \n",
      "----\n",
      "iter 45000, loss: 49.980364912470804\n",
      "----\n",
      " he it dor where the. toups adt tade hro the lut but the thaneeed is sture in ithide perechs theeleling sicgy deid, uf expravere the toexmem dacid do us at ytupd youl thitere to the oneike so sigtmicg  \n",
      "----\n",
      "iter 46000, loss: 49.95036463778507\n",
      "----\n",
      "  wang chibtranming cat whos's're ise omeabimeticte sody fompvaltenme't you'and quurly. wit be ith dornonteel be ind cory got pit on A wer in pome , A hangal pas tu canthech, fireead of fif mA uanmer r \n",
      "----\n",
      "iter 47000, loss: 49.85079520498669\n",
      "----\n",
      " heay oy do getanvey at fining thyereo Whethe You'nge me jurs reale thely and the angelled dontherof ind sow a that's they'rlee. for dassevere gake omea to you't deld didech meest ge ine the, mo tors t \n",
      "----\n",
      "iter 48000, loss: 49.827068233772934\n",
      "----\n",
      " d forl betry leunanw imh what stamcsas the wonst thabe be ays fore to cans ann be doisey sike that rich Ind cis recangmunty exin tis letis cinavat. ke be nouse had co're mon the be be have to yuuch su \n",
      "----\n",
      "iter 49000, loss: 49.72008356513182\n"
     ]
    }
   ],
   "source": [
    "n, p = 0, 0\n",
    "iters = 50000\n",
    "\n",
    "while n < iters:\n",
    "    # prepare inputs to go from left-to-right in steps (seq_len)\n",
    "    if p + seq_len + 1 >= len(data) or n == 0:\n",
    "        hprev = np.zeros((hidden_size, 1))  # reset RNN memory\n",
    "        p = 0  # start at beginning of data\n",
    "    inputs = [ctoi[ch] for ch in data[p : p + seq_len]]\n",
    "    targets = [ctoi[ch] for ch in data[p + 1 : p + seq_len + 1]]\n",
    "\n",
    "    if n % 1000 == 0:\n",
    "        sample_ix = sample(hprev, inputs[0], 200)\n",
    "        txt = \"\".join(itoc[ix] for ix in sample_ix)\n",
    "        print(f\"----\\n {txt} \\n----\")\n",
    "\n",
    "    # forward seq_len characters through the net and fetch gradient\n",
    "    loss, dw_xh, dw_hh, dw_hy, db_h, db_y, hprev = calc_loss(inputs, targets, hprev)\n",
    "    smooth_loss = smooth_loss * 0.999 + loss * 0.001  # TODO: why is this good?\n",
    "\n",
    "    if n % 1000 == 0:\n",
    "        print(f\"iter {n}, loss: {smooth_loss}\")\n",
    "\n",
    "    # Adagrad parameter update\n",
    "    for param, dparam, mem in zip(\n",
    "        [w_xh, w_hh, w_hy, b_h, b_y],\n",
    "        [dw_xh, dw_hh, dw_hy, db_h, db_y],\n",
    "        [mw_xh, mw_hh, mw_hy, mb_h, mb_y],\n",
    "    ):\n",
    "        mem += dparam * dparam\n",
    "        param += -learning_rate * dparam / np.sqrt(mem + 1e-8)\n",
    "    p += seq_len\n",
    "    n += 1\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
